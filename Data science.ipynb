{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPwfxxPqFpiDop+VpmJ91Y2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/munga21407/data_analysis/blob/main/Data%20science.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_and_clean_data(sales_data_path, weather_data_path, merge_on=None):\n",
        "    # Load sales data\n",
        "    sales_data = pd.read_csv(sales_data_path) if sales_data_path.endswith('.csv') else pd.read_json(sales_data_path)\n",
        "\n",
        "    # Load weather data\n",
        "    weather_data = pd.read_csv(weather_data_path) if weather_data_path.endswith('.csv') else pd.read_json(weather_data_path)\n",
        "\n",
        "    # Basic cleaning for sales data\n",
        "    sales_data.dropna(inplace=True)  # Drop rows with missing values\n",
        "    sales_data['timestamp'] = pd.to_datetime(sales_data['timestamp'])  # Convert timestamp to datetime if not already\n",
        "\n",
        "    # Basic cleaning for weather data\n",
        "    weather_data.dropna(inplace=True)  # Drop rows with missing values\n",
        "    weather_data['timestamp'] = pd.to_datetime(weather_data['timestamp'])  # Convert timestamp to datetime if not already\n",
        "\n",
        "    # If merge_on is specified, merge weather data with sales data\n",
        "    if merge_on:\n",
        "        merged_data = pd.merge(sales_data, weather_data, on=merge_on)\n",
        "        return merged_data\n",
        "    else:\n",
        "        return sales_data, weather_data  # Return separate dataframes if merge_on is not specified\n",
        "\n",
        "# Example usage:\n",
        "sales_path = 'path_to_sales_data.csv'\n",
        "weather_path = 'path_to_weather_data.csv'\n",
        "merged_data = load_and_clean_data(sales_path, weather_path, merge_on=['location', 'timestamp'])\n",
        "print(merged_data.head())\n"
      ],
      "metadata": {
        "id": "11d5He5T8LgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_features(df):\n",
        "    # Rolling sales averages\n",
        "    df['rolling_sales_avg'] = df.groupby('location')['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())\n",
        "\n",
        "    # Lagged weather features\n",
        "    df['lagged_rain'] = df.groupby('location')['rain'].shift(2)  # Sales two days after a rainstorm\n",
        "\n",
        "    # Categorical variables for holidays, seasons, etc.\n",
        "    # For demonstration purposes, assuming a simple approach for illustration\n",
        "    df['is_holiday'] = df['timestamp'].dt.date.isin(holiday_dates)\n",
        "    df['season'] = df['timestamp'].dt.month.map({1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
        "                                                 6: 'Summer', 7: 'Summer', 8: 'Summer', 9: 'Fall', 10: 'Fall',\n",
        "                                                 11: 'Fall', 12: 'Winter'})\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "cleaned_df = load_and_clean_data(sales_data_path, weather_data_path, merge_on=['location', 'timestamp'])\n",
        "feature_enriched_df = create_features(cleaned_df)\n",
        "print(feature_enriched_df.head())\n"
      ],
      "metadata": {
        "id": "alxAhjM784rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_data(df, target_col, test_size)\n",
        "\n",
        "Splits the DataFrame into training and testing sets.\n",
        "target_col indicates the column you're predicting (e.g., 'units_sold').\n",
        "test_size determines the proportion held out for validation."
      ],
      "metadata": {
        "id": "leS68rMP8_Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def train_model(X_train, y_train, model_type):\n",
        "    if model_type == \"linear_regression\":\n",
        "        model = LinearRegression()\n",
        "    elif model_type == \"random_forest\":\n",
        "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type. Supported types are 'linear_regression' and 'random_forest'.\")\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X_train and y_train are already defined\n",
        "trained_model = train_model(X_train, y_train, model_type=\"random_forest\")\n"
      ],
      "metadata": {
        "id": "K0sDOgCR9KPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def forecast(model, X_new, forecast_horizon):\n",
        "    if forecast_horizon == 'daily':\n",
        "        forecasts = model.predict(X_new)\n",
        "    elif forecast_horizon == 'weekly':\n",
        "        # Assuming X_new contains daily data, we'll aggregate predictions to weekly level\n",
        "        num_weeks = int(np.ceil(len(X_new) / 7))\n",
        "        forecasts = []\n",
        "        for i in range(num_weeks):\n",
        "            start_index = i * 7\n",
        "            end_index = min((i + 1) * 7, len(X_new))\n",
        "            weekly_pred = model.predict(X_new[start_index:end_index])\n",
        "            forecasts.append(weekly_pred.sum())  # Summing up daily predictions for the week\n",
        "    else:\n",
        "        raise ValueError(\"Invalid forecast horizon. Supported horizons are 'daily' and 'weekly'.\")\n",
        "\n",
        "    return forecasts\n",
        "\n",
        "# Example usage:\n",
        "# Assuming trained_model and X_new are already defined\n",
        "sales_forecasts_daily = forecast(trained_model, X_new, forecast_horizon='daily')\n",
        "sales_forecasts_weekly = forecast(trained_model, X_new, forecast_horizon='weekly')\n"
      ],
      "metadata": {
        "id": "_zVNOpf09Wjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_inventory(forecasts, lead_times, safety_stock_levels, reorder_point_multiplier=2):\n",
        "    recommended_stock_levels = []\n",
        "\n",
        "    for forecast, lead_time, safety_stock in zip(forecasts, lead_times, safety_stock_levels):\n",
        "        # Calculate reorder point based on forecasted sales and lead time\n",
        "        reorder_point = forecast * lead_time + safety_stock * reorder_point_multiplier\n",
        "        recommended_stock_levels.append(reorder_point)\n",
        "\n",
        "    return recommended_stock_levels\n",
        "\n",
        "# Example usage:\n",
        "forecasts = [100, 150, 120]  # Example forecasted sales\n",
        "lead_times = [7, 14, 10]  # Example lead times for restocking\n",
        "safety_stock_levels = [20, 30, 25]  # Example safety stock levels\n",
        "recommended_stock_levels = optimize_inventory(forecasts, lead_times, safety_stock_levels)\n",
        "print(recommended_stock_levels)\n"
      ],
      "metadata": {
        "id": "SPWCZMWn9isB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}